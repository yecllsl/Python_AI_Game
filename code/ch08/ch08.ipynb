{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x20f1bf5eb50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.linspace(-2,2,100)\n",
    "y = x**2\n",
    "plt.plot(x,y)\n",
    "plt.scatter(0,0,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1.60, Y:4.00, gx:4.00\n",
      "X:1.28, Y:2.56, gx:3.20\n",
      "X:1.02, Y:1.64, gx:2.56\n",
      "X:0.82, Y:1.05, gx:2.05\n",
      "X:0.66, Y:0.67, gx:1.64\n",
      "X:0.52, Y:0.43, gx:1.31\n",
      "X:0.42, Y:0.27, gx:1.05\n",
      "X:0.34, Y:0.18, gx:0.84\n",
      "X:0.27, Y:0.11, gx:0.67\n",
      "X:0.21, Y:0.07, gx:0.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21474836480000006"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def min_gradient(x_start, rate, n_iter, f,g):\n",
    "    # 初始化变量，x为起始点\n",
    "    x = x_start\n",
    "    # 开始迭代，最多进行n_iter次\n",
    "    for n in range(n_iter):\n",
    "        # 计算当前点的梯度，即函数在当前点的导数，可以算出斜率\n",
    "        gx = g(x)\n",
    "        # 计算当前点的函数值\n",
    "        y = f(x)\n",
    "        # 根据学习率和梯度更新x的值，\n",
    "        x = x - rate*gx # 梯度下降\n",
    "        # 打印当前迭代的信息\n",
    "        print(\"X:{x:.2f}, Y:{y:.2f}, gx:{gx:.2f}\".format(x=x, y=y,gx=gx))\n",
    "        # 如果梯度绝对值小于0.0001，认为已经收敛，提前结束迭代\n",
    "        if abs(gx)<0.0001:\n",
    "            break\n",
    "    # 返回最终的x值\n",
    "    return x\n",
    "\n",
    "f = lambda x:x**2\n",
    "g = lambda x: 2*x\n",
    "min_gradient(x_start=2,rate=0.1, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:0.80, Y:4.00, gx:4.00\n",
      "X:0.32, Y:0.64, gx:1.60\n",
      "X:0.13, Y:0.10, gx:0.64\n",
      "X:0.05, Y:0.02, gx:0.26\n",
      "X:0.02, Y:0.00, gx:0.10\n",
      "X:0.01, Y:0.00, gx:0.04\n",
      "X:0.00, Y:0.00, gx:0.02\n",
      "X:0.00, Y:0.00, gx:0.01\n",
      "X:0.00, Y:0.00, gx:0.00\n",
      "X:0.00, Y:0.00, gx:0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00020971520000000014"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_gradient(x_start=2,rate=0.3, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:-2.40, Y:4.00, gx:4.00\n",
      "X:2.88, Y:5.76, gx:-4.80\n",
      "X:-3.46, Y:8.29, gx:5.76\n",
      "X:4.15, Y:11.94, gx:-6.91\n",
      "X:-4.98, Y:17.20, gx:8.29\n",
      "X:5.97, Y:24.77, gx:-9.95\n",
      "X:-7.17, Y:35.66, gx:11.94\n",
      "X:8.60, Y:51.36, gx:-14.33\n",
      "X:-10.32, Y:73.95, gx:17.20\n",
      "X:12.38, Y:106.49, gx:-20.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.383472844800014"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_gradient(x_start=2,rate=1.1, n_iter=10,f=f,g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.ones(5, 3,dtype=torch.float64)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6243, -0.6118, -0.5282],\n",
      "        [-1.0730,  0.8654, -2.3015],\n",
      "        [ 1.7448, -0.7612,  0.3190],\n",
      "        [-0.2494,  1.4621, -2.0601],\n",
      "        [-0.3224, -0.3841,  1.1338]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "y_array= np.random.randn(5,3)\n",
    "y_tensor = torch.from_numpy(y_array)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6243,  0.3882,  0.4718],\n",
      "        [-0.0730,  1.8654, -1.3015],\n",
      "        [ 2.7448,  0.2388,  1.3190],\n",
      "        [ 0.7506,  2.4621, -1.0601],\n",
      "        [ 0.6776,  0.6159,  2.1338]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "result = torch.add(x_tensor,y_tensor)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6243,  0.3882,  0.4718],\n",
      "        [-0.0730,  1.8654, -1.3015],\n",
      "        [ 2.7448,  0.2388,  1.3190],\n",
      "        [ 0.7506,  2.4621, -1.0601],\n",
      "        [ 0.6776,  0.6159,  2.1338]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_tensor.add_(y_tensor)\n",
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614, 0.2669, 0.0617, 0.6213]])\n",
      "tensor([[-0.4519],\n",
      "        [-0.1661],\n",
      "        [-1.5228],\n",
      "        [ 0.3817]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "x_tensor = torch.randn(1, 4)\n",
    "print(x_tensor)\n",
    "y_tensor = torch.randn(4, 1)\n",
    "print(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x_tensor,y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2669])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 8]' is invalid for input of size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m z_tensor = \u001b[43mx_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      2\u001b[39m z_tensor.shape\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 8]' is invalid for input of size 4"
     ]
    }
   ],
   "source": [
    "z_tensor = x_tensor.view(-1, 8) \n",
    "z_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mz_tensor\u001b[49m.tolist()\n\u001b[32m      2\u001b[39m z_tensor.numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'z_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "z_tensor.tolist()\n",
    "z_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True) # 定义一个值为3的tensor名字叫作x\n",
    "y = x*x   # 定义y和x的关系\n",
    "y.backward()  # 自动计算梯度\n",
    "print(x.grad) #打印y相对于x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "z = x*x\n",
    "z.backward()  \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "z = x*x\n",
    "z.backward()  \n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gred(x_start, rate, num, f):\n",
    "    # 将初始值转换为PyTorch张量，并设置需要计算梯度\n",
    "    x = torch.tensor(x_start, requires_grad=True)\n",
    "    # 开始迭代，最多进行num次\n",
    "    for n in range(num): \n",
    "        # 计算当前x对应的函数值\n",
    "        y = f(x)\n",
    "        # 自动计算梯度\n",
    "        y.backward()   # 计算梯度\n",
    "        # 获取当前梯度值并转换为Python列表\n",
    "        gx = x.grad.tolist()  # 取出梯度\n",
    "        # 根据学习率和梯度更新x的值\n",
    "        newx = x.tolist() - rate*gx # 修正\n",
    "        # 将更新后的值重新转换为PyTorch张量，并设置需要计算梯度\n",
    "        x = torch.tensor(newx, requires_grad=True)  #重新定义x, 所以不需要清零操作\n",
    "        # 打印当前迭代的信息\n",
    "        print(\"X:{x:.2f}, Y:{y:.2f},gx:{gx:.2f}\".format(x=x, y=y,gx=gx))\n",
    "        # 如果梯度绝对值小于0.0001，认为已经收敛，提前结束迭代\n",
    "        if abs(gx)<0.0001:\n",
    "            break\n",
    "    # 返回最终的x值\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1.60, Y:4.00,gx:4.00\n",
      "X:1.28, Y:2.56,gx:3.20\n",
      "X:1.02, Y:1.64,gx:2.56\n",
      "X:0.82, Y:1.05,gx:2.05\n",
      "X:0.66, Y:0.67,gx:1.64\n",
      "X:0.52, Y:0.43,gx:1.31\n",
      "X:0.42, Y:0.27,gx:1.05\n",
      "X:0.34, Y:0.18,gx:0.84\n",
      "X:0.27, Y:0.11,gx:0.67\n",
      "X:0.21, Y:0.07,gx:0.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2147, requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x:x**2\n",
    "min_gred(2.0, 0.1, 10,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[20,2])\n",
    "y = 2*X[:,0] + 2*X[:,1]\n",
    "y = y.reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Simple:\n",
    "\n",
    "    def __init__(self, dim,rate=0.1, n_iter=20):\n",
    "        self.rate = rate  # 步长，学习速率\n",
    "        self.n_iter = n_iter  # 迭代次数\n",
    "        self.W = np.random.randn(1, dim)  # 代表被训练的系数\n",
    "        self.MSE = []   # 用于保存损失的空list\n",
    "\n",
    "    def fit(self, X, y):  # 训练函数\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.predict(X)  # 计算预测的Y\n",
    "            errors = y - output  \n",
    "            g = np.dot(errors.T, X)\n",
    "            self.W += self.rate * g  # 根据更新规则更新系数\n",
    "            self.MSE.append((errors**2).sum())  # 记录损失函数的值\n",
    "            #print(self.W)\n",
    "\n",
    "\n",
    "    def predict(self, X):   # 给定系数和X计算预测的Y\n",
    "        output = np.dot(X, self.W.T)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = NeuralNet_Simple(dim = X.shape[1], rate=0.001, n_iter=30)\n",
    "simple_nn.fit(X, y);  # 喂入数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.99634945 2.00372808]]\n"
     ]
    }
   ],
   "source": [
    "print(simple_nn.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(15368.929119959024),\n",
       " np.float64(1479.6097345509363),\n",
       " np.float64(195.01271617740633),\n",
       " np.float64(60.731465471329194),\n",
       " np.float64(35.96332622174337),\n",
       " np.float64(24.80872469950572),\n",
       " np.float64(17.49441405288127),\n",
       " np.float64(12.371955124020007),\n",
       " np.float64(8.75259918179382),\n",
       " np.float64(6.192360448838486),\n",
       " np.float64(4.381048295164106),\n",
       " np.float64(3.0995610145066257),\n",
       " np.float64(2.19291794779831),\n",
       " np.float64(1.5514742779608692),\n",
       " np.float64(1.097657320702395),\n",
       " np.float64(0.7765849624829105),\n",
       " np.float64(0.5494284897447095),\n",
       " np.float64(0.3887168564004128),\n",
       " np.float64(0.275014487363213),\n",
       " np.float64(0.19457084768597196),\n",
       " np.float64(0.1376575289986064),\n",
       " np.float64(0.09739174966532545),\n",
       " np.float64(0.06890398928321081),\n",
       " np.float64(0.04874909584698737),\n",
       " np.float64(0.03448964814114135),\n",
       " np.float64(0.02440118751398621),\n",
       " np.float64(0.017263671396592982),\n",
       " np.float64(0.012213928109797883),\n",
       " np.float64(0.008641269660679135),\n",
       " np.float64(0.0061136385180354455)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.00396065])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.predict([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[20,2])\n",
    "y_area = X[:,0]*X[:,1]\n",
    "y_area = y_area.reshape(20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = NeuralNet_Simple(dim = X.shape[1], rate=0.001, n_iter=20)\n",
    "simple_nn.fit(X, y);  # 喂入数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(28188.705572166116),\n",
       " np.float64(3893.5853695534493),\n",
       " np.float64(2855.541842043533),\n",
       " np.float64(2796.611634431257),\n",
       " np.float64(2783.160525489939),\n",
       " np.float64(2774.8180430558896),\n",
       " np.float64(2768.9598644171265),\n",
       " np.float64(2764.8135207965247),\n",
       " np.float64(2761.87741410713),\n",
       " np.float64(2759.7982421048473),\n",
       " np.float64(2758.3258966646817),\n",
       " np.float64(2757.283269454177),\n",
       " np.float64(2756.5449430556037),\n",
       " np.float64(2756.022104340556),\n",
       " np.float64(2755.6518611493493),\n",
       " np.float64(2755.389677005963),\n",
       " np.float64(2755.204013834631),\n",
       " np.float64(2755.072538251311),\n",
       " np.float64(2754.979435085587),\n",
       " np.float64(2754.91350497713)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.7099672])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn.predict([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "n_iter = 30\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size, output_size, bias=False)\n",
    "# 定义损失函数和最优化方法\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 500.5566\n",
      "Epoch [2/30], Loss: 418.7619\n",
      "Epoch [3/30], Loss: 355.4742\n",
      "Epoch [4/30], Loss: 306.5039\n",
      "Epoch [5/30], Loss: 268.6100\n",
      "Epoch [6/30], Loss: 239.2852\n",
      "Epoch [7/30], Loss: 216.5898\n",
      "Epoch [8/30], Loss: 199.0231\n",
      "Epoch [9/30], Loss: 185.4246\n",
      "Epoch [10/30], Loss: 174.8960\n",
      "Epoch [11/30], Loss: 166.7425\n",
      "Epoch [12/30], Loss: 160.4268\n",
      "Epoch [13/30], Loss: 155.5331\n",
      "Epoch [14/30], Loss: 151.7396\n",
      "Epoch [15/30], Loss: 148.7976\n",
      "Epoch [16/30], Loss: 146.5144\n",
      "Epoch [17/30], Loss: 144.7412\n",
      "Epoch [18/30], Loss: 143.3627\n",
      "Epoch [19/30], Loss: 142.2898\n",
      "Epoch [20/30], Loss: 141.4535\n",
      "Epoch [21/30], Loss: 140.8004\n",
      "Epoch [22/30], Loss: 140.2892\n",
      "Epoch [23/30], Loss: 139.8881\n",
      "Epoch [24/30], Loss: 139.5722\n",
      "Epoch [25/30], Loss: 139.3224\n",
      "Epoch [26/30], Loss: 139.1240\n",
      "Epoch [27/30], Loss: 138.9655\n",
      "Epoch [28/30], Loss: 138.8380\n",
      "Epoch [29/30], Loss: 138.7346\n",
      "Epoch [30/30], Loss: 138.6500\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    # 数据转换\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y).to(torch.float)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.6123, 1.8668]], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码主要使用NumPy库生成随机数据并计算面积。让我逐行解释：\n",
    "\n",
    "X = np.random.randint(1,10,[100,2])\n",
    "\n",
    "使用NumPy的randint函数生成一个100行2列的二维数组\n",
    "数组中的每个元素都是1到10之间的随机整数\n",
    "可以理解为生成了100个矩形的长和宽\n",
    "y_area = X[:,0]*X[:,1]\n",
    "\n",
    "计算每个矩形的面积\n",
    "X[:,0]获取所有行的第一列（长）\n",
    "X[:,1]获取所有行的第二列（宽）\n",
    "通过逐元素相乘得到面积\n",
    "y_area= y_area.reshape(100,1)\n",
    "\n",
    "将面积数组从一维（100,）重塑为二维（100,1）\n",
    "这样每个面积值都是一个单独的数组元素，便于后续处理\n",
    "总结：这段代码生成了100个随机矩形，计算了它们的面积，并将面积值整理成适合机器学习模型输入的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1,10,[100,2])\n",
    "y_area = X[:,0]*X[:,1]\n",
    "y_area= y_area.reshape(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个神经网络模型的超参数。让我逐行解释：\n",
    "\n",
    "input_size = 2\n",
    "\n",
    "输入层的大小为2，对应之前代码中每个矩形的长和宽两个特征\n",
    "hidden_size = 50\n",
    "\n",
    "隐藏层的大小为50，表示隐藏层有50个神经元\n",
    "这是一个中等大小的隐藏层，适合处理相对简单的任务\n",
    "output_size = 1\n",
    "\n",
    "输出层的大小为1，因为我们只需要预测一个值（面积）\n",
    "n_iter = 1000\n",
    "\n",
    "训练迭代次数为1000次\n",
    "这是一个相对较大的值，适合确保模型充分训练\n",
    "learning_rate = 0.01\n",
    "\n",
    "学习率为0.01\n",
    "这是一个常用的初始学习率值，既不会太大导致训练不稳定，也不会太小导致训练过慢\n",
    "这些超参数共同定义了神经网络的结构和训练过程，为后续的模型构建和训练做好准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "# 定义超参数\n",
    "input_size = 2\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "n_iter = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 调用父类构造函数\n",
    "        super().__init__()\n",
    "        # 定义第一层全连接层：输入层到隐藏层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        # 定义激活函数为ReLU\n",
    "        self.activate = nn.ReLU()\n",
    "        # 定义第二层全连接层：隐藏层到输出层\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过第一层全连接层\n",
    "        out = self.fc1(x)\n",
    "        # 应用ReLU激活函数\n",
    "        out = self.activate(out)\n",
    "        # 通过第二层全连接层\n",
    "        out = self.fc2(out)\n",
    "        # 返回最终输出\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化神经网络模型，这里使用的是一个简单的三层神经网络\n",
    "# 参数 input_size: 输入层维度，hidden_size: 隐藏层维度，output_size: 输出层维度\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# 定义损失函数，这里使用的是均方误差损失函数\n",
    "criterion = nn.MSELoss()  \n",
    "\n",
    "# 定义优化器，这里选择的是Adam优化器\n",
    "# 参数 model.parameters(): 需要优化的模型参数，lr: 学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 852.7946\n",
      "Epoch [2/1000], Loss: 795.6735\n",
      "Epoch [3/1000], Loss: 740.4216\n",
      "Epoch [4/1000], Loss: 686.7975\n",
      "Epoch [5/1000], Loss: 634.7451\n",
      "Epoch [6/1000], Loss: 584.2558\n",
      "Epoch [7/1000], Loss: 535.4365\n",
      "Epoch [8/1000], Loss: 488.2063\n",
      "Epoch [9/1000], Loss: 442.8104\n",
      "Epoch [10/1000], Loss: 399.3736\n",
      "Epoch [11/1000], Loss: 358.2023\n",
      "Epoch [12/1000], Loss: 319.5795\n",
      "Epoch [13/1000], Loss: 283.6925\n",
      "Epoch [14/1000], Loss: 251.1169\n",
      "Epoch [15/1000], Loss: 222.2209\n",
      "Epoch [16/1000], Loss: 197.1601\n",
      "Epoch [17/1000], Loss: 176.4167\n",
      "Epoch [18/1000], Loss: 160.1949\n",
      "Epoch [19/1000], Loss: 148.5327\n",
      "Epoch [20/1000], Loss: 141.4561\n",
      "Epoch [21/1000], Loss: 138.5866\n",
      "Epoch [22/1000], Loss: 139.2707\n",
      "Epoch [23/1000], Loss: 142.6090\n",
      "Epoch [24/1000], Loss: 147.5088\n",
      "Epoch [25/1000], Loss: 152.8077\n",
      "Epoch [26/1000], Loss: 157.4312\n",
      "Epoch [27/1000], Loss: 160.5874\n",
      "Epoch [28/1000], Loss: 161.8476\n",
      "Epoch [29/1000], Loss: 161.1242\n",
      "Epoch [30/1000], Loss: 158.6176\n",
      "Epoch [31/1000], Loss: 154.7286\n",
      "Epoch [32/1000], Loss: 149.9383\n",
      "Epoch [33/1000], Loss: 144.7510\n",
      "Epoch [34/1000], Loss: 139.6206\n",
      "Epoch [35/1000], Loss: 134.9098\n",
      "Epoch [36/1000], Loss: 130.8700\n",
      "Epoch [37/1000], Loss: 127.6465\n",
      "Epoch [38/1000], Loss: 125.2516\n",
      "Epoch [39/1000], Loss: 123.6223\n",
      "Epoch [40/1000], Loss: 122.6407\n",
      "Epoch [41/1000], Loss: 122.1593\n",
      "Epoch [42/1000], Loss: 122.0124\n",
      "Epoch [43/1000], Loss: 122.0343\n",
      "Epoch [44/1000], Loss: 122.0801\n",
      "Epoch [45/1000], Loss: 122.0633\n",
      "Epoch [46/1000], Loss: 121.8661\n",
      "Epoch [47/1000], Loss: 121.4348\n",
      "Epoch [48/1000], Loss: 120.7661\n",
      "Epoch [49/1000], Loss: 119.8627\n",
      "Epoch [50/1000], Loss: 118.7324\n",
      "Epoch [51/1000], Loss: 117.4215\n",
      "Epoch [52/1000], Loss: 115.9982\n",
      "Epoch [53/1000], Loss: 114.5166\n",
      "Epoch [54/1000], Loss: 113.0283\n",
      "Epoch [55/1000], Loss: 111.5883\n",
      "Epoch [56/1000], Loss: 110.2373\n",
      "Epoch [57/1000], Loss: 109.0144\n",
      "Epoch [58/1000], Loss: 107.9112\n",
      "Epoch [59/1000], Loss: 106.9018\n",
      "Epoch [60/1000], Loss: 105.9533\n",
      "Epoch [61/1000], Loss: 105.0550\n",
      "Epoch [62/1000], Loss: 104.1453\n",
      "Epoch [63/1000], Loss: 103.1895\n",
      "Epoch [64/1000], Loss: 102.1707\n",
      "Epoch [65/1000], Loss: 101.0949\n",
      "Epoch [66/1000], Loss: 99.9449\n",
      "Epoch [67/1000], Loss: 98.7163\n",
      "Epoch [68/1000], Loss: 97.4350\n",
      "Epoch [69/1000], Loss: 96.1437\n",
      "Epoch [70/1000], Loss: 94.8339\n",
      "Epoch [71/1000], Loss: 93.5482\n",
      "Epoch [72/1000], Loss: 92.2939\n",
      "Epoch [73/1000], Loss: 91.0467\n",
      "Epoch [74/1000], Loss: 89.8042\n",
      "Epoch [75/1000], Loss: 88.5659\n",
      "Epoch [76/1000], Loss: 87.3105\n",
      "Epoch [77/1000], Loss: 86.0267\n",
      "Epoch [78/1000], Loss: 84.7114\n",
      "Epoch [79/1000], Loss: 83.3639\n",
      "Epoch [80/1000], Loss: 81.9742\n",
      "Epoch [81/1000], Loss: 80.5443\n",
      "Epoch [82/1000], Loss: 79.0788\n",
      "Epoch [83/1000], Loss: 77.5854\n",
      "Epoch [84/1000], Loss: 76.0726\n",
      "Epoch [85/1000], Loss: 74.5697\n",
      "Epoch [86/1000], Loss: 73.0651\n",
      "Epoch [87/1000], Loss: 71.5542\n",
      "Epoch [88/1000], Loss: 70.0379\n",
      "Epoch [89/1000], Loss: 68.5147\n",
      "Epoch [90/1000], Loss: 66.9825\n",
      "Epoch [91/1000], Loss: 65.4402\n",
      "Epoch [92/1000], Loss: 63.8920\n",
      "Epoch [93/1000], Loss: 62.3450\n",
      "Epoch [94/1000], Loss: 60.7973\n",
      "Epoch [95/1000], Loss: 59.2518\n",
      "Epoch [96/1000], Loss: 57.7121\n",
      "Epoch [97/1000], Loss: 56.1834\n",
      "Epoch [98/1000], Loss: 54.6664\n",
      "Epoch [99/1000], Loss: 53.1680\n",
      "Epoch [100/1000], Loss: 51.6834\n",
      "Epoch [101/1000], Loss: 50.2141\n",
      "Epoch [102/1000], Loss: 48.7609\n",
      "Epoch [103/1000], Loss: 47.3267\n",
      "Epoch [104/1000], Loss: 45.9128\n",
      "Epoch [105/1000], Loss: 44.5176\n",
      "Epoch [106/1000], Loss: 43.1435\n",
      "Epoch [107/1000], Loss: 41.7940\n",
      "Epoch [108/1000], Loss: 40.4765\n",
      "Epoch [109/1000], Loss: 39.1879\n",
      "Epoch [110/1000], Loss: 37.9329\n",
      "Epoch [111/1000], Loss: 36.7131\n",
      "Epoch [112/1000], Loss: 35.5294\n",
      "Epoch [113/1000], Loss: 34.3854\n",
      "Epoch [114/1000], Loss: 33.2772\n",
      "Epoch [115/1000], Loss: 32.2061\n",
      "Epoch [116/1000], Loss: 31.1777\n",
      "Epoch [117/1000], Loss: 30.1927\n",
      "Epoch [118/1000], Loss: 29.2489\n",
      "Epoch [119/1000], Loss: 28.3468\n",
      "Epoch [120/1000], Loss: 27.4863\n",
      "Epoch [121/1000], Loss: 26.6673\n",
      "Epoch [122/1000], Loss: 25.8933\n",
      "Epoch [123/1000], Loss: 25.1595\n",
      "Epoch [124/1000], Loss: 24.4652\n",
      "Epoch [125/1000], Loss: 23.8117\n",
      "Epoch [126/1000], Loss: 23.1996\n",
      "Epoch [127/1000], Loss: 22.6297\n",
      "Epoch [128/1000], Loss: 22.0933\n",
      "Epoch [129/1000], Loss: 21.5906\n",
      "Epoch [130/1000], Loss: 21.1231\n",
      "Epoch [131/1000], Loss: 20.6913\n",
      "Epoch [132/1000], Loss: 20.2906\n",
      "Epoch [133/1000], Loss: 19.9231\n",
      "Epoch [134/1000], Loss: 19.5799\n",
      "Epoch [135/1000], Loss: 19.2644\n",
      "Epoch [136/1000], Loss: 18.9729\n",
      "Epoch [137/1000], Loss: 18.7036\n",
      "Epoch [138/1000], Loss: 18.4550\n",
      "Epoch [139/1000], Loss: 18.2246\n",
      "Epoch [140/1000], Loss: 18.0107\n",
      "Epoch [141/1000], Loss: 17.8118\n",
      "Epoch [142/1000], Loss: 17.6261\n",
      "Epoch [143/1000], Loss: 17.4521\n",
      "Epoch [144/1000], Loss: 17.2883\n",
      "Epoch [145/1000], Loss: 17.1334\n",
      "Epoch [146/1000], Loss: 16.9862\n",
      "Epoch [147/1000], Loss: 16.8457\n",
      "Epoch [148/1000], Loss: 16.7110\n",
      "Epoch [149/1000], Loss: 16.5811\n",
      "Epoch [150/1000], Loss: 16.4552\n",
      "Epoch [151/1000], Loss: 16.3326\n",
      "Epoch [152/1000], Loss: 16.2100\n",
      "Epoch [153/1000], Loss: 16.0850\n",
      "Epoch [154/1000], Loss: 15.9603\n",
      "Epoch [155/1000], Loss: 15.8362\n",
      "Epoch [156/1000], Loss: 15.7105\n",
      "Epoch [157/1000], Loss: 15.5646\n",
      "Epoch [158/1000], Loss: 15.4074\n",
      "Epoch [159/1000], Loss: 15.2470\n",
      "Epoch [160/1000], Loss: 15.0378\n",
      "Epoch [161/1000], Loss: 14.8028\n",
      "Epoch [162/1000], Loss: 14.5564\n",
      "Epoch [163/1000], Loss: 14.3264\n",
      "Epoch [164/1000], Loss: 14.1595\n",
      "Epoch [165/1000], Loss: 13.9918\n",
      "Epoch [166/1000], Loss: 13.8306\n",
      "Epoch [167/1000], Loss: 13.6740\n",
      "Epoch [168/1000], Loss: 13.5244\n",
      "Epoch [169/1000], Loss: 13.3844\n",
      "Epoch [170/1000], Loss: 13.2522\n",
      "Epoch [171/1000], Loss: 13.1251\n",
      "Epoch [172/1000], Loss: 12.9965\n",
      "Epoch [173/1000], Loss: 12.8623\n",
      "Epoch [174/1000], Loss: 12.7463\n",
      "Epoch [175/1000], Loss: 12.6343\n",
      "Epoch [176/1000], Loss: 12.5249\n",
      "Epoch [177/1000], Loss: 12.4287\n",
      "Epoch [178/1000], Loss: 12.3336\n",
      "Epoch [179/1000], Loss: 12.2381\n",
      "Epoch [180/1000], Loss: 12.1432\n",
      "Epoch [181/1000], Loss: 12.0499\n",
      "Epoch [182/1000], Loss: 11.9591\n",
      "Epoch [183/1000], Loss: 11.8701\n",
      "Epoch [184/1000], Loss: 11.7821\n",
      "Epoch [185/1000], Loss: 11.6944\n",
      "Epoch [186/1000], Loss: 11.6075\n",
      "Epoch [187/1000], Loss: 11.5231\n",
      "Epoch [188/1000], Loss: 11.4536\n",
      "Epoch [189/1000], Loss: 11.3871\n",
      "Epoch [190/1000], Loss: 11.3240\n",
      "Epoch [191/1000], Loss: 11.2603\n",
      "Epoch [192/1000], Loss: 11.1987\n",
      "Epoch [193/1000], Loss: 11.1386\n",
      "Epoch [194/1000], Loss: 11.0789\n",
      "Epoch [195/1000], Loss: 11.0206\n",
      "Epoch [196/1000], Loss: 10.9636\n",
      "Epoch [197/1000], Loss: 10.9082\n",
      "Epoch [198/1000], Loss: 10.8552\n",
      "Epoch [199/1000], Loss: 10.8048\n",
      "Epoch [200/1000], Loss: 10.7562\n",
      "Epoch [201/1000], Loss: 10.7084\n",
      "Epoch [202/1000], Loss: 10.6612\n",
      "Epoch [203/1000], Loss: 10.6148\n",
      "Epoch [204/1000], Loss: 10.5696\n",
      "Epoch [205/1000], Loss: 10.5270\n",
      "Epoch [206/1000], Loss: 10.4856\n",
      "Epoch [207/1000], Loss: 10.4444\n",
      "Epoch [208/1000], Loss: 10.4035\n",
      "Epoch [209/1000], Loss: 10.3632\n",
      "Epoch [210/1000], Loss: 10.3222\n",
      "Epoch [211/1000], Loss: 10.2826\n",
      "Epoch [212/1000], Loss: 10.2430\n",
      "Epoch [213/1000], Loss: 10.2031\n",
      "Epoch [214/1000], Loss: 10.1629\n",
      "Epoch [215/1000], Loss: 10.1232\n",
      "Epoch [216/1000], Loss: 10.0832\n",
      "Epoch [217/1000], Loss: 10.0435\n",
      "Epoch [218/1000], Loss: 10.0051\n",
      "Epoch [219/1000], Loss: 9.9691\n",
      "Epoch [220/1000], Loss: 9.9341\n",
      "Epoch [221/1000], Loss: 9.8998\n",
      "Epoch [222/1000], Loss: 9.8672\n",
      "Epoch [223/1000], Loss: 9.8351\n",
      "Epoch [224/1000], Loss: 9.8038\n",
      "Epoch [225/1000], Loss: 9.7728\n",
      "Epoch [226/1000], Loss: 9.7420\n",
      "Epoch [227/1000], Loss: 9.7114\n",
      "Epoch [228/1000], Loss: 9.6811\n",
      "Epoch [229/1000], Loss: 9.6514\n",
      "Epoch [230/1000], Loss: 9.6211\n",
      "Epoch [231/1000], Loss: 9.5910\n",
      "Epoch [232/1000], Loss: 9.5612\n",
      "Epoch [233/1000], Loss: 9.5314\n",
      "Epoch [234/1000], Loss: 9.5015\n",
      "Epoch [235/1000], Loss: 9.4713\n",
      "Epoch [236/1000], Loss: 9.4415\n",
      "Epoch [237/1000], Loss: 9.4117\n",
      "Epoch [238/1000], Loss: 9.3818\n",
      "Epoch [239/1000], Loss: 9.3522\n",
      "Epoch [240/1000], Loss: 9.3225\n",
      "Epoch [241/1000], Loss: 9.2921\n",
      "Epoch [242/1000], Loss: 9.2621\n",
      "Epoch [243/1000], Loss: 9.2315\n",
      "Epoch [244/1000], Loss: 9.2016\n",
      "Epoch [245/1000], Loss: 9.1716\n",
      "Epoch [246/1000], Loss: 9.1415\n",
      "Epoch [247/1000], Loss: 9.1112\n",
      "Epoch [248/1000], Loss: 9.0809\n",
      "Epoch [249/1000], Loss: 9.0508\n",
      "Epoch [250/1000], Loss: 9.0224\n",
      "Epoch [251/1000], Loss: 8.9930\n",
      "Epoch [252/1000], Loss: 8.9629\n",
      "Epoch [253/1000], Loss: 8.9333\n",
      "Epoch [254/1000], Loss: 8.9048\n",
      "Epoch [255/1000], Loss: 8.8758\n",
      "Epoch [256/1000], Loss: 8.8465\n",
      "Epoch [257/1000], Loss: 8.8177\n",
      "Epoch [258/1000], Loss: 8.7907\n",
      "Epoch [259/1000], Loss: 8.7629\n",
      "Epoch [260/1000], Loss: 8.7364\n",
      "Epoch [261/1000], Loss: 8.7103\n",
      "Epoch [262/1000], Loss: 8.6840\n",
      "Epoch [263/1000], Loss: 8.6579\n",
      "Epoch [264/1000], Loss: 8.6330\n",
      "Epoch [265/1000], Loss: 8.6084\n",
      "Epoch [266/1000], Loss: 8.5849\n",
      "Epoch [267/1000], Loss: 8.5613\n",
      "Epoch [268/1000], Loss: 8.5378\n",
      "Epoch [269/1000], Loss: 8.5147\n",
      "Epoch [270/1000], Loss: 8.4915\n",
      "Epoch [271/1000], Loss: 8.4691\n",
      "Epoch [272/1000], Loss: 8.4465\n",
      "Epoch [273/1000], Loss: 8.4238\n",
      "Epoch [274/1000], Loss: 8.4015\n",
      "Epoch [275/1000], Loss: 8.3794\n",
      "Epoch [276/1000], Loss: 8.3570\n",
      "Epoch [277/1000], Loss: 8.3358\n",
      "Epoch [278/1000], Loss: 8.3143\n",
      "Epoch [279/1000], Loss: 8.2926\n",
      "Epoch [280/1000], Loss: 8.2707\n",
      "Epoch [281/1000], Loss: 8.2494\n",
      "Epoch [282/1000], Loss: 8.2286\n",
      "Epoch [283/1000], Loss: 8.2073\n",
      "Epoch [284/1000], Loss: 8.1860\n",
      "Epoch [285/1000], Loss: 8.1653\n",
      "Epoch [286/1000], Loss: 8.1444\n",
      "Epoch [287/1000], Loss: 8.1240\n",
      "Epoch [288/1000], Loss: 8.1034\n",
      "Epoch [289/1000], Loss: 8.0826\n",
      "Epoch [290/1000], Loss: 8.0623\n",
      "Epoch [291/1000], Loss: 8.0419\n",
      "Epoch [292/1000], Loss: 8.0211\n",
      "Epoch [293/1000], Loss: 8.0003\n",
      "Epoch [294/1000], Loss: 7.9800\n",
      "Epoch [295/1000], Loss: 7.9594\n",
      "Epoch [296/1000], Loss: 7.9392\n",
      "Epoch [297/1000], Loss: 7.9191\n",
      "Epoch [298/1000], Loss: 7.8986\n",
      "Epoch [299/1000], Loss: 7.8784\n",
      "Epoch [300/1000], Loss: 7.8587\n",
      "Epoch [301/1000], Loss: 7.8385\n",
      "Epoch [302/1000], Loss: 7.8186\n",
      "Epoch [303/1000], Loss: 7.7986\n",
      "Epoch [304/1000], Loss: 7.7785\n",
      "Epoch [305/1000], Loss: 7.7591\n",
      "Epoch [306/1000], Loss: 7.7394\n",
      "Epoch [307/1000], Loss: 7.7190\n",
      "Epoch [308/1000], Loss: 7.6984\n",
      "Epoch [309/1000], Loss: 7.6777\n",
      "Epoch [310/1000], Loss: 7.6566\n",
      "Epoch [311/1000], Loss: 7.6352\n",
      "Epoch [312/1000], Loss: 7.6140\n",
      "Epoch [313/1000], Loss: 7.5926\n",
      "Epoch [314/1000], Loss: 7.5719\n",
      "Epoch [315/1000], Loss: 7.5517\n",
      "Epoch [316/1000], Loss: 7.5307\n",
      "Epoch [317/1000], Loss: 7.5105\n",
      "Epoch [318/1000], Loss: 7.4906\n",
      "Epoch [319/1000], Loss: 7.4703\n",
      "Epoch [320/1000], Loss: 7.4500\n",
      "Epoch [321/1000], Loss: 7.4302\n",
      "Epoch [322/1000], Loss: 7.4107\n",
      "Epoch [323/1000], Loss: 7.3908\n",
      "Epoch [324/1000], Loss: 7.3706\n",
      "Epoch [325/1000], Loss: 7.3516\n",
      "Epoch [326/1000], Loss: 7.3330\n",
      "Epoch [327/1000], Loss: 7.3143\n",
      "Epoch [328/1000], Loss: 7.2956\n",
      "Epoch [329/1000], Loss: 7.2766\n",
      "Epoch [330/1000], Loss: 7.2579\n",
      "Epoch [331/1000], Loss: 7.2397\n",
      "Epoch [332/1000], Loss: 7.2214\n",
      "Epoch [333/1000], Loss: 7.2027\n",
      "Epoch [334/1000], Loss: 7.1840\n",
      "Epoch [335/1000], Loss: 7.1652\n",
      "Epoch [336/1000], Loss: 7.1469\n",
      "Epoch [337/1000], Loss: 7.1281\n",
      "Epoch [338/1000], Loss: 7.1095\n",
      "Epoch [339/1000], Loss: 7.0912\n",
      "Epoch [340/1000], Loss: 7.0724\n",
      "Epoch [341/1000], Loss: 7.0532\n",
      "Epoch [342/1000], Loss: 7.0349\n",
      "Epoch [343/1000], Loss: 7.0161\n",
      "Epoch [344/1000], Loss: 6.9970\n",
      "Epoch [345/1000], Loss: 6.9788\n",
      "Epoch [346/1000], Loss: 6.9602\n",
      "Epoch [347/1000], Loss: 6.9418\n",
      "Epoch [348/1000], Loss: 6.9226\n",
      "Epoch [349/1000], Loss: 6.9034\n",
      "Epoch [350/1000], Loss: 6.8852\n",
      "Epoch [351/1000], Loss: 6.8670\n",
      "Epoch [352/1000], Loss: 6.8489\n",
      "Epoch [353/1000], Loss: 6.8302\n",
      "Epoch [354/1000], Loss: 6.8123\n",
      "Epoch [355/1000], Loss: 6.7940\n",
      "Epoch [356/1000], Loss: 6.7766\n",
      "Epoch [357/1000], Loss: 6.7590\n",
      "Epoch [358/1000], Loss: 6.7410\n",
      "Epoch [359/1000], Loss: 6.7238\n",
      "Epoch [360/1000], Loss: 6.7063\n",
      "Epoch [361/1000], Loss: 6.6877\n",
      "Epoch [362/1000], Loss: 6.6665\n",
      "Epoch [363/1000], Loss: 6.6444\n",
      "Epoch [364/1000], Loss: 6.6223\n",
      "Epoch [365/1000], Loss: 6.6003\n",
      "Epoch [366/1000], Loss: 6.5770\n",
      "Epoch [367/1000], Loss: 6.5535\n",
      "Epoch [368/1000], Loss: 6.5291\n",
      "Epoch [369/1000], Loss: 6.5050\n",
      "Epoch [370/1000], Loss: 6.4806\n",
      "Epoch [371/1000], Loss: 6.4556\n",
      "Epoch [372/1000], Loss: 6.4283\n",
      "Epoch [373/1000], Loss: 6.4011\n",
      "Epoch [374/1000], Loss: 6.3746\n",
      "Epoch [375/1000], Loss: 6.3464\n",
      "Epoch [376/1000], Loss: 6.3174\n",
      "Epoch [377/1000], Loss: 6.2872\n",
      "Epoch [378/1000], Loss: 6.2579\n",
      "Epoch [379/1000], Loss: 6.2285\n",
      "Epoch [380/1000], Loss: 6.1990\n",
      "Epoch [381/1000], Loss: 6.1692\n",
      "Epoch [382/1000], Loss: 6.1407\n",
      "Epoch [383/1000], Loss: 6.1124\n",
      "Epoch [384/1000], Loss: 6.0842\n",
      "Epoch [385/1000], Loss: 6.0555\n",
      "Epoch [386/1000], Loss: 6.0273\n",
      "Epoch [387/1000], Loss: 5.9998\n",
      "Epoch [388/1000], Loss: 5.9721\n",
      "Epoch [389/1000], Loss: 5.9443\n",
      "Epoch [390/1000], Loss: 5.9162\n",
      "Epoch [391/1000], Loss: 5.8872\n",
      "Epoch [392/1000], Loss: 5.8568\n",
      "Epoch [393/1000], Loss: 5.8254\n",
      "Epoch [394/1000], Loss: 5.7964\n",
      "Epoch [395/1000], Loss: 5.7674\n",
      "Epoch [396/1000], Loss: 5.7381\n",
      "Epoch [397/1000], Loss: 5.7085\n",
      "Epoch [398/1000], Loss: 5.6789\n",
      "Epoch [399/1000], Loss: 5.6498\n",
      "Epoch [400/1000], Loss: 5.6202\n",
      "Epoch [401/1000], Loss: 5.5905\n",
      "Epoch [402/1000], Loss: 5.5617\n",
      "Epoch [403/1000], Loss: 5.5325\n",
      "Epoch [404/1000], Loss: 5.5047\n",
      "Epoch [405/1000], Loss: 5.4767\n",
      "Epoch [406/1000], Loss: 5.4486\n",
      "Epoch [407/1000], Loss: 5.4192\n",
      "Epoch [408/1000], Loss: 5.3929\n",
      "Epoch [409/1000], Loss: 5.3698\n",
      "Epoch [410/1000], Loss: 5.3462\n",
      "Epoch [411/1000], Loss: 5.3218\n",
      "Epoch [412/1000], Loss: 5.2971\n",
      "Epoch [413/1000], Loss: 5.2720\n",
      "Epoch [414/1000], Loss: 5.2478\n",
      "Epoch [415/1000], Loss: 5.2238\n",
      "Epoch [416/1000], Loss: 5.2000\n",
      "Epoch [417/1000], Loss: 5.1771\n",
      "Epoch [418/1000], Loss: 5.1542\n",
      "Epoch [419/1000], Loss: 5.1313\n",
      "Epoch [420/1000], Loss: 5.1102\n",
      "Epoch [421/1000], Loss: 5.0887\n",
      "Epoch [422/1000], Loss: 5.0670\n",
      "Epoch [423/1000], Loss: 5.0454\n",
      "Epoch [424/1000], Loss: 5.0243\n",
      "Epoch [425/1000], Loss: 5.0030\n",
      "Epoch [426/1000], Loss: 4.9817\n",
      "Epoch [427/1000], Loss: 4.9601\n",
      "Epoch [428/1000], Loss: 4.9387\n",
      "Epoch [429/1000], Loss: 4.9181\n",
      "Epoch [430/1000], Loss: 4.8975\n",
      "Epoch [431/1000], Loss: 4.8763\n",
      "Epoch [432/1000], Loss: 4.8557\n",
      "Epoch [433/1000], Loss: 4.8362\n",
      "Epoch [434/1000], Loss: 4.8165\n",
      "Epoch [435/1000], Loss: 4.7963\n",
      "Epoch [436/1000], Loss: 4.7764\n",
      "Epoch [437/1000], Loss: 4.7562\n",
      "Epoch [438/1000], Loss: 4.7366\n",
      "Epoch [439/1000], Loss: 4.7165\n",
      "Epoch [440/1000], Loss: 4.6963\n",
      "Epoch [441/1000], Loss: 4.6755\n",
      "Epoch [442/1000], Loss: 4.6549\n",
      "Epoch [443/1000], Loss: 4.6337\n",
      "Epoch [444/1000], Loss: 4.6135\n",
      "Epoch [445/1000], Loss: 4.5931\n",
      "Epoch [446/1000], Loss: 4.5725\n",
      "Epoch [447/1000], Loss: 4.5522\n",
      "Epoch [448/1000], Loss: 4.5327\n",
      "Epoch [449/1000], Loss: 4.5130\n",
      "Epoch [450/1000], Loss: 4.4938\n",
      "Epoch [451/1000], Loss: 4.4741\n",
      "Epoch [452/1000], Loss: 4.4540\n",
      "Epoch [453/1000], Loss: 4.4333\n",
      "Epoch [454/1000], Loss: 4.4140\n",
      "Epoch [455/1000], Loss: 4.3948\n",
      "Epoch [456/1000], Loss: 4.3751\n",
      "Epoch [457/1000], Loss: 4.3549\n",
      "Epoch [458/1000], Loss: 4.3354\n",
      "Epoch [459/1000], Loss: 4.3153\n",
      "Epoch [460/1000], Loss: 4.2960\n",
      "Epoch [461/1000], Loss: 4.2769\n",
      "Epoch [462/1000], Loss: 4.2573\n",
      "Epoch [463/1000], Loss: 4.2376\n",
      "Epoch [464/1000], Loss: 4.2180\n",
      "Epoch [465/1000], Loss: 4.1979\n",
      "Epoch [466/1000], Loss: 4.1776\n",
      "Epoch [467/1000], Loss: 4.1584\n",
      "Epoch [468/1000], Loss: 4.1398\n",
      "Epoch [469/1000], Loss: 4.1210\n",
      "Epoch [470/1000], Loss: 4.1026\n",
      "Epoch [471/1000], Loss: 4.0844\n",
      "Epoch [472/1000], Loss: 4.0668\n",
      "Epoch [473/1000], Loss: 4.0490\n",
      "Epoch [474/1000], Loss: 4.0313\n",
      "Epoch [475/1000], Loss: 4.0141\n",
      "Epoch [476/1000], Loss: 3.9966\n",
      "Epoch [477/1000], Loss: 3.9798\n",
      "Epoch [478/1000], Loss: 3.9625\n",
      "Epoch [479/1000], Loss: 3.9457\n",
      "Epoch [480/1000], Loss: 3.9288\n",
      "Epoch [481/1000], Loss: 3.9120\n",
      "Epoch [482/1000], Loss: 3.8950\n",
      "Epoch [483/1000], Loss: 3.8778\n",
      "Epoch [484/1000], Loss: 3.8616\n",
      "Epoch [485/1000], Loss: 3.8450\n",
      "Epoch [486/1000], Loss: 3.8280\n",
      "Epoch [487/1000], Loss: 3.8109\n",
      "Epoch [488/1000], Loss: 3.7940\n",
      "Epoch [489/1000], Loss: 3.7770\n",
      "Epoch [490/1000], Loss: 3.7601\n",
      "Epoch [491/1000], Loss: 3.7427\n",
      "Epoch [492/1000], Loss: 3.7266\n",
      "Epoch [493/1000], Loss: 3.7094\n",
      "Epoch [494/1000], Loss: 3.6925\n",
      "Epoch [495/1000], Loss: 3.6755\n",
      "Epoch [496/1000], Loss: 3.6595\n",
      "Epoch [497/1000], Loss: 3.6439\n",
      "Epoch [498/1000], Loss: 3.6283\n",
      "Epoch [499/1000], Loss: 3.6128\n",
      "Epoch [500/1000], Loss: 3.5975\n",
      "Epoch [501/1000], Loss: 3.5824\n",
      "Epoch [502/1000], Loss: 3.5673\n",
      "Epoch [503/1000], Loss: 3.5519\n",
      "Epoch [504/1000], Loss: 3.5371\n",
      "Epoch [505/1000], Loss: 3.5223\n",
      "Epoch [506/1000], Loss: 3.5073\n",
      "Epoch [507/1000], Loss: 3.4923\n",
      "Epoch [508/1000], Loss: 3.4775\n",
      "Epoch [509/1000], Loss: 3.4624\n",
      "Epoch [510/1000], Loss: 3.4478\n",
      "Epoch [511/1000], Loss: 3.4333\n",
      "Epoch [512/1000], Loss: 3.4187\n",
      "Epoch [513/1000], Loss: 3.4040\n",
      "Epoch [514/1000], Loss: 3.3894\n",
      "Epoch [515/1000], Loss: 3.3749\n",
      "Epoch [516/1000], Loss: 3.3604\n",
      "Epoch [517/1000], Loss: 3.3457\n",
      "Epoch [518/1000], Loss: 3.3312\n",
      "Epoch [519/1000], Loss: 3.3168\n",
      "Epoch [520/1000], Loss: 3.3022\n",
      "Epoch [521/1000], Loss: 3.2876\n",
      "Epoch [522/1000], Loss: 3.2730\n",
      "Epoch [523/1000], Loss: 3.2583\n",
      "Epoch [524/1000], Loss: 3.2432\n",
      "Epoch [525/1000], Loss: 3.2279\n",
      "Epoch [526/1000], Loss: 3.2125\n",
      "Epoch [527/1000], Loss: 3.1971\n",
      "Epoch [528/1000], Loss: 3.1815\n",
      "Epoch [529/1000], Loss: 3.1659\n",
      "Epoch [530/1000], Loss: 3.1504\n",
      "Epoch [531/1000], Loss: 3.1347\n",
      "Epoch [532/1000], Loss: 3.1192\n",
      "Epoch [533/1000], Loss: 3.1035\n",
      "Epoch [534/1000], Loss: 3.0879\n",
      "Epoch [535/1000], Loss: 3.0722\n",
      "Epoch [536/1000], Loss: 3.0564\n",
      "Epoch [537/1000], Loss: 3.0407\n",
      "Epoch [538/1000], Loss: 3.0252\n",
      "Epoch [539/1000], Loss: 3.0098\n",
      "Epoch [540/1000], Loss: 2.9943\n",
      "Epoch [541/1000], Loss: 2.9790\n",
      "Epoch [542/1000], Loss: 2.9637\n",
      "Epoch [543/1000], Loss: 2.9485\n",
      "Epoch [544/1000], Loss: 2.9333\n",
      "Epoch [545/1000], Loss: 2.9181\n",
      "Epoch [546/1000], Loss: 2.9029\n",
      "Epoch [547/1000], Loss: 2.8878\n",
      "Epoch [548/1000], Loss: 2.8727\n",
      "Epoch [549/1000], Loss: 2.8578\n",
      "Epoch [550/1000], Loss: 2.8428\n",
      "Epoch [551/1000], Loss: 2.8278\n",
      "Epoch [552/1000], Loss: 2.8131\n",
      "Epoch [553/1000], Loss: 2.7983\n",
      "Epoch [554/1000], Loss: 2.7834\n",
      "Epoch [555/1000], Loss: 2.7687\n",
      "Epoch [556/1000], Loss: 2.7540\n",
      "Epoch [557/1000], Loss: 2.7393\n",
      "Epoch [558/1000], Loss: 2.7244\n",
      "Epoch [559/1000], Loss: 2.7097\n",
      "Epoch [560/1000], Loss: 2.6948\n",
      "Epoch [561/1000], Loss: 2.6800\n",
      "Epoch [562/1000], Loss: 2.6653\n",
      "Epoch [563/1000], Loss: 2.6506\n",
      "Epoch [564/1000], Loss: 2.6359\n",
      "Epoch [565/1000], Loss: 2.6212\n",
      "Epoch [566/1000], Loss: 2.6066\n",
      "Epoch [567/1000], Loss: 2.5919\n",
      "Epoch [568/1000], Loss: 2.5773\n",
      "Epoch [569/1000], Loss: 2.5630\n",
      "Epoch [570/1000], Loss: 2.5486\n",
      "Epoch [571/1000], Loss: 2.5342\n",
      "Epoch [572/1000], Loss: 2.5200\n",
      "Epoch [573/1000], Loss: 2.5057\n",
      "Epoch [574/1000], Loss: 2.4914\n",
      "Epoch [575/1000], Loss: 2.4772\n",
      "Epoch [576/1000], Loss: 2.4629\n",
      "Epoch [577/1000], Loss: 2.4487\n",
      "Epoch [578/1000], Loss: 2.4345\n",
      "Epoch [579/1000], Loss: 2.4206\n",
      "Epoch [580/1000], Loss: 2.4066\n",
      "Epoch [581/1000], Loss: 2.3926\n",
      "Epoch [582/1000], Loss: 2.3786\n",
      "Epoch [583/1000], Loss: 2.3647\n",
      "Epoch [584/1000], Loss: 2.3509\n",
      "Epoch [585/1000], Loss: 2.3371\n",
      "Epoch [586/1000], Loss: 2.3235\n",
      "Epoch [587/1000], Loss: 2.3099\n",
      "Epoch [588/1000], Loss: 2.2963\n",
      "Epoch [589/1000], Loss: 2.2828\n",
      "Epoch [590/1000], Loss: 2.2693\n",
      "Epoch [591/1000], Loss: 2.2560\n",
      "Epoch [592/1000], Loss: 2.2428\n",
      "Epoch [593/1000], Loss: 2.2295\n",
      "Epoch [594/1000], Loss: 2.2162\n",
      "Epoch [595/1000], Loss: 2.2031\n",
      "Epoch [596/1000], Loss: 2.1898\n",
      "Epoch [597/1000], Loss: 2.1764\n",
      "Epoch [598/1000], Loss: 2.1631\n",
      "Epoch [599/1000], Loss: 2.1498\n",
      "Epoch [600/1000], Loss: 2.1366\n",
      "Epoch [601/1000], Loss: 2.1235\n",
      "Epoch [602/1000], Loss: 2.1104\n",
      "Epoch [603/1000], Loss: 2.0972\n",
      "Epoch [604/1000], Loss: 2.0843\n",
      "Epoch [605/1000], Loss: 2.0714\n",
      "Epoch [606/1000], Loss: 2.0585\n",
      "Epoch [607/1000], Loss: 2.0457\n",
      "Epoch [608/1000], Loss: 2.0329\n",
      "Epoch [609/1000], Loss: 2.0200\n",
      "Epoch [610/1000], Loss: 2.0073\n",
      "Epoch [611/1000], Loss: 1.9946\n",
      "Epoch [612/1000], Loss: 1.9819\n",
      "Epoch [613/1000], Loss: 1.9694\n",
      "Epoch [614/1000], Loss: 1.9571\n",
      "Epoch [615/1000], Loss: 1.9447\n",
      "Epoch [616/1000], Loss: 1.9322\n",
      "Epoch [617/1000], Loss: 1.9194\n",
      "Epoch [618/1000], Loss: 1.9066\n",
      "Epoch [619/1000], Loss: 1.8939\n",
      "Epoch [620/1000], Loss: 1.8814\n",
      "Epoch [621/1000], Loss: 1.8690\n",
      "Epoch [622/1000], Loss: 1.8566\n",
      "Epoch [623/1000], Loss: 1.8441\n",
      "Epoch [624/1000], Loss: 1.8315\n",
      "Epoch [625/1000], Loss: 1.8190\n",
      "Epoch [626/1000], Loss: 1.8065\n",
      "Epoch [627/1000], Loss: 1.7940\n",
      "Epoch [628/1000], Loss: 1.7816\n",
      "Epoch [629/1000], Loss: 1.7693\n",
      "Epoch [630/1000], Loss: 1.7571\n",
      "Epoch [631/1000], Loss: 1.7449\n",
      "Epoch [632/1000], Loss: 1.7328\n",
      "Epoch [633/1000], Loss: 1.7207\n",
      "Epoch [634/1000], Loss: 1.7087\n",
      "Epoch [635/1000], Loss: 1.6969\n",
      "Epoch [636/1000], Loss: 1.6851\n",
      "Epoch [637/1000], Loss: 1.6736\n",
      "Epoch [638/1000], Loss: 1.6622\n",
      "Epoch [639/1000], Loss: 1.6509\n",
      "Epoch [640/1000], Loss: 1.6397\n",
      "Epoch [641/1000], Loss: 1.6285\n",
      "Epoch [642/1000], Loss: 1.6177\n",
      "Epoch [643/1000], Loss: 1.6071\n",
      "Epoch [644/1000], Loss: 1.5965\n",
      "Epoch [645/1000], Loss: 1.5861\n",
      "Epoch [646/1000], Loss: 1.5758\n",
      "Epoch [647/1000], Loss: 1.5655\n",
      "Epoch [648/1000], Loss: 1.5554\n",
      "Epoch [649/1000], Loss: 1.5455\n",
      "Epoch [650/1000], Loss: 1.5358\n",
      "Epoch [651/1000], Loss: 1.5262\n",
      "Epoch [652/1000], Loss: 1.5167\n",
      "Epoch [653/1000], Loss: 1.5072\n",
      "Epoch [654/1000], Loss: 1.4978\n",
      "Epoch [655/1000], Loss: 1.4885\n",
      "Epoch [656/1000], Loss: 1.4791\n",
      "Epoch [657/1000], Loss: 1.4698\n",
      "Epoch [658/1000], Loss: 1.4605\n",
      "Epoch [659/1000], Loss: 1.4515\n",
      "Epoch [660/1000], Loss: 1.4440\n",
      "Epoch [661/1000], Loss: 1.4344\n",
      "Epoch [662/1000], Loss: 1.4261\n",
      "Epoch [663/1000], Loss: 1.4177\n",
      "Epoch [664/1000], Loss: 1.4094\n",
      "Epoch [665/1000], Loss: 1.4010\n",
      "Epoch [666/1000], Loss: 1.3923\n",
      "Epoch [667/1000], Loss: 1.3843\n",
      "Epoch [668/1000], Loss: 1.3760\n",
      "Epoch [669/1000], Loss: 1.3677\n",
      "Epoch [670/1000], Loss: 1.3596\n",
      "Epoch [671/1000], Loss: 1.3517\n",
      "Epoch [672/1000], Loss: 1.3438\n",
      "Epoch [673/1000], Loss: 1.3359\n",
      "Epoch [674/1000], Loss: 1.3286\n",
      "Epoch [675/1000], Loss: 1.3205\n",
      "Epoch [676/1000], Loss: 1.3127\n",
      "Epoch [677/1000], Loss: 1.3052\n",
      "Epoch [678/1000], Loss: 1.2970\n",
      "Epoch [679/1000], Loss: 1.2902\n",
      "Epoch [680/1000], Loss: 1.2824\n",
      "Epoch [681/1000], Loss: 1.2748\n",
      "Epoch [682/1000], Loss: 1.2675\n",
      "Epoch [683/1000], Loss: 1.2608\n",
      "Epoch [684/1000], Loss: 1.2541\n",
      "Epoch [685/1000], Loss: 1.2472\n",
      "Epoch [686/1000], Loss: 1.2402\n",
      "Epoch [687/1000], Loss: 1.2328\n",
      "Epoch [688/1000], Loss: 1.2259\n",
      "Epoch [689/1000], Loss: 1.2192\n",
      "Epoch [690/1000], Loss: 1.2135\n",
      "Epoch [691/1000], Loss: 1.2059\n",
      "Epoch [692/1000], Loss: 1.1994\n",
      "Epoch [693/1000], Loss: 1.1928\n",
      "Epoch [694/1000], Loss: 1.1866\n",
      "Epoch [695/1000], Loss: 1.1801\n",
      "Epoch [696/1000], Loss: 1.1733\n",
      "Epoch [697/1000], Loss: 1.1670\n",
      "Epoch [698/1000], Loss: 1.1610\n",
      "Epoch [699/1000], Loss: 1.1546\n",
      "Epoch [700/1000], Loss: 1.1484\n",
      "Epoch [701/1000], Loss: 1.1439\n",
      "Epoch [702/1000], Loss: 1.1365\n",
      "Epoch [703/1000], Loss: 1.1312\n",
      "Epoch [704/1000], Loss: 1.1253\n",
      "Epoch [705/1000], Loss: 1.1192\n",
      "Epoch [706/1000], Loss: 1.1139\n",
      "Epoch [707/1000], Loss: 1.1083\n",
      "Epoch [708/1000], Loss: 1.1025\n",
      "Epoch [709/1000], Loss: 1.0974\n",
      "Epoch [710/1000], Loss: 1.0915\n",
      "Epoch [711/1000], Loss: 1.0861\n",
      "Epoch [712/1000], Loss: 1.0809\n",
      "Epoch [713/1000], Loss: 1.0761\n",
      "Epoch [714/1000], Loss: 1.0710\n",
      "Epoch [715/1000], Loss: 1.0666\n",
      "Epoch [716/1000], Loss: 1.0616\n",
      "Epoch [717/1000], Loss: 1.0564\n",
      "Epoch [718/1000], Loss: 1.0520\n",
      "Epoch [719/1000], Loss: 1.0470\n",
      "Epoch [720/1000], Loss: 1.0427\n",
      "Epoch [721/1000], Loss: 1.0378\n",
      "Epoch [722/1000], Loss: 1.0342\n",
      "Epoch [723/1000], Loss: 1.0288\n",
      "Epoch [724/1000], Loss: 1.0252\n",
      "Epoch [725/1000], Loss: 1.0214\n",
      "Epoch [726/1000], Loss: 1.0170\n",
      "Epoch [727/1000], Loss: 1.0125\n",
      "Epoch [728/1000], Loss: 1.0076\n",
      "Epoch [729/1000], Loss: 1.0039\n",
      "Epoch [730/1000], Loss: 0.9997\n",
      "Epoch [731/1000], Loss: 0.9955\n",
      "Epoch [732/1000], Loss: 0.9913\n",
      "Epoch [733/1000], Loss: 0.9876\n",
      "Epoch [734/1000], Loss: 0.9833\n",
      "Epoch [735/1000], Loss: 0.9788\n",
      "Epoch [736/1000], Loss: 0.9753\n",
      "Epoch [737/1000], Loss: 0.9713\n",
      "Epoch [738/1000], Loss: 0.9675\n",
      "Epoch [739/1000], Loss: 0.9634\n",
      "Epoch [740/1000], Loss: 0.9595\n",
      "Epoch [741/1000], Loss: 0.9557\n",
      "Epoch [742/1000], Loss: 0.9523\n",
      "Epoch [743/1000], Loss: 0.9483\n",
      "Epoch [744/1000], Loss: 0.9456\n",
      "Epoch [745/1000], Loss: 0.9413\n",
      "Epoch [746/1000], Loss: 0.9382\n",
      "Epoch [747/1000], Loss: 0.9354\n",
      "Epoch [748/1000], Loss: 0.9317\n",
      "Epoch [749/1000], Loss: 0.9277\n",
      "Epoch [750/1000], Loss: 0.9236\n",
      "Epoch [751/1000], Loss: 0.9214\n",
      "Epoch [752/1000], Loss: 0.9182\n",
      "Epoch [753/1000], Loss: 0.9136\n",
      "Epoch [754/1000], Loss: 0.9109\n",
      "Epoch [755/1000], Loss: 0.9084\n",
      "Epoch [756/1000], Loss: 0.9057\n",
      "Epoch [757/1000], Loss: 0.9015\n",
      "Epoch [758/1000], Loss: 0.8973\n",
      "Epoch [759/1000], Loss: 0.8930\n",
      "Epoch [760/1000], Loss: 0.8916\n",
      "Epoch [761/1000], Loss: 0.8892\n",
      "Epoch [762/1000], Loss: 0.8846\n",
      "Epoch [763/1000], Loss: 0.8788\n",
      "Epoch [764/1000], Loss: 0.8770\n",
      "Epoch [765/1000], Loss: 0.8743\n",
      "Epoch [766/1000], Loss: 0.8718\n",
      "Epoch [767/1000], Loss: 0.8687\n",
      "Epoch [768/1000], Loss: 0.8648\n",
      "Epoch [769/1000], Loss: 0.8609\n",
      "Epoch [770/1000], Loss: 0.8592\n",
      "Epoch [771/1000], Loss: 0.8573\n",
      "Epoch [772/1000], Loss: 0.8534\n",
      "Epoch [773/1000], Loss: 0.8483\n",
      "Epoch [774/1000], Loss: 0.8457\n",
      "Epoch [775/1000], Loss: 0.8428\n",
      "Epoch [776/1000], Loss: 0.8394\n",
      "Epoch [777/1000], Loss: 0.8366\n",
      "Epoch [778/1000], Loss: 0.8337\n",
      "Epoch [779/1000], Loss: 0.8308\n",
      "Epoch [780/1000], Loss: 0.8280\n",
      "Epoch [781/1000], Loss: 0.8253\n",
      "Epoch [782/1000], Loss: 0.8223\n",
      "Epoch [783/1000], Loss: 0.8199\n",
      "Epoch [784/1000], Loss: 0.8171\n",
      "Epoch [785/1000], Loss: 0.8151\n",
      "Epoch [786/1000], Loss: 0.8126\n",
      "Epoch [787/1000], Loss: 0.8101\n",
      "Epoch [788/1000], Loss: 0.8071\n",
      "Epoch [789/1000], Loss: 0.8043\n",
      "Epoch [790/1000], Loss: 0.8022\n",
      "Epoch [791/1000], Loss: 0.7993\n",
      "Epoch [792/1000], Loss: 0.7970\n",
      "Epoch [793/1000], Loss: 0.7945\n",
      "Epoch [794/1000], Loss: 0.7920\n",
      "Epoch [795/1000], Loss: 0.7902\n",
      "Epoch [796/1000], Loss: 0.7879\n",
      "Epoch [797/1000], Loss: 0.7853\n",
      "Epoch [798/1000], Loss: 0.7830\n",
      "Epoch [799/1000], Loss: 0.7812\n",
      "Epoch [800/1000], Loss: 0.7786\n",
      "Epoch [801/1000], Loss: 0.7767\n",
      "Epoch [802/1000], Loss: 0.7743\n",
      "Epoch [803/1000], Loss: 0.7725\n",
      "Epoch [804/1000], Loss: 0.7703\n",
      "Epoch [805/1000], Loss: 0.7681\n",
      "Epoch [806/1000], Loss: 0.7663\n",
      "Epoch [807/1000], Loss: 0.7636\n",
      "Epoch [808/1000], Loss: 0.7626\n",
      "Epoch [809/1000], Loss: 0.7606\n",
      "Epoch [810/1000], Loss: 0.7580\n",
      "Epoch [811/1000], Loss: 0.7562\n",
      "Epoch [812/1000], Loss: 0.7542\n",
      "Epoch [813/1000], Loss: 0.7520\n",
      "Epoch [814/1000], Loss: 0.7498\n",
      "Epoch [815/1000], Loss: 0.7485\n",
      "Epoch [816/1000], Loss: 0.7463\n",
      "Epoch [817/1000], Loss: 0.7448\n",
      "Epoch [818/1000], Loss: 0.7432\n",
      "Epoch [819/1000], Loss: 0.7404\n",
      "Epoch [820/1000], Loss: 0.7401\n",
      "Epoch [821/1000], Loss: 0.7386\n",
      "Epoch [822/1000], Loss: 0.7364\n",
      "Epoch [823/1000], Loss: 0.7338\n",
      "Epoch [824/1000], Loss: 0.7326\n",
      "Epoch [825/1000], Loss: 0.7315\n",
      "Epoch [826/1000], Loss: 0.7297\n",
      "Epoch [827/1000], Loss: 0.7266\n",
      "Epoch [828/1000], Loss: 0.7253\n",
      "Epoch [829/1000], Loss: 0.7245\n",
      "Epoch [830/1000], Loss: 0.7223\n",
      "Epoch [831/1000], Loss: 0.7196\n",
      "Epoch [832/1000], Loss: 0.7187\n",
      "Epoch [833/1000], Loss: 0.7178\n",
      "Epoch [834/1000], Loss: 0.7160\n",
      "Epoch [835/1000], Loss: 0.7136\n",
      "Epoch [836/1000], Loss: 0.7112\n",
      "Epoch [837/1000], Loss: 0.7110\n",
      "Epoch [838/1000], Loss: 0.7100\n",
      "Epoch [839/1000], Loss: 0.7080\n",
      "Epoch [840/1000], Loss: 0.7052\n",
      "Epoch [841/1000], Loss: 0.7038\n",
      "Epoch [842/1000], Loss: 0.7032\n",
      "Epoch [843/1000], Loss: 0.7017\n",
      "Epoch [844/1000], Loss: 0.6994\n",
      "Epoch [845/1000], Loss: 0.6970\n",
      "Epoch [846/1000], Loss: 0.6968\n",
      "Epoch [847/1000], Loss: 0.6960\n",
      "Epoch [848/1000], Loss: 0.6942\n",
      "Epoch [849/1000], Loss: 0.6915\n",
      "Epoch [850/1000], Loss: 0.6899\n",
      "Epoch [851/1000], Loss: 0.6894\n",
      "Epoch [852/1000], Loss: 0.6882\n",
      "Epoch [853/1000], Loss: 0.6862\n",
      "Epoch [854/1000], Loss: 0.6839\n",
      "Epoch [855/1000], Loss: 0.6832\n",
      "Epoch [856/1000], Loss: 0.6824\n",
      "Epoch [857/1000], Loss: 0.6806\n",
      "Epoch [858/1000], Loss: 0.6780\n",
      "Epoch [859/1000], Loss: 0.6777\n",
      "Epoch [860/1000], Loss: 0.6772\n",
      "Epoch [861/1000], Loss: 0.6760\n",
      "Epoch [862/1000], Loss: 0.6740\n",
      "Epoch [863/1000], Loss: 0.6717\n",
      "Epoch [864/1000], Loss: 0.6705\n",
      "Epoch [865/1000], Loss: 0.6697\n",
      "Epoch [866/1000], Loss: 0.6679\n",
      "Epoch [867/1000], Loss: 0.6663\n",
      "Epoch [868/1000], Loss: 0.6653\n",
      "Epoch [869/1000], Loss: 0.6642\n",
      "Epoch [870/1000], Loss: 0.6625\n",
      "Epoch [871/1000], Loss: 0.6608\n",
      "Epoch [872/1000], Loss: 0.6602\n",
      "Epoch [873/1000], Loss: 0.6592\n",
      "Epoch [874/1000], Loss: 0.6573\n",
      "Epoch [875/1000], Loss: 0.6561\n",
      "Epoch [876/1000], Loss: 0.6552\n",
      "Epoch [877/1000], Loss: 0.6539\n",
      "Epoch [878/1000], Loss: 0.6520\n",
      "Epoch [879/1000], Loss: 0.6514\n",
      "Epoch [880/1000], Loss: 0.6506\n",
      "Epoch [881/1000], Loss: 0.6487\n",
      "Epoch [882/1000], Loss: 0.6473\n",
      "Epoch [883/1000], Loss: 0.6466\n",
      "Epoch [884/1000], Loss: 0.6453\n",
      "Epoch [885/1000], Loss: 0.6436\n",
      "Epoch [886/1000], Loss: 0.6427\n",
      "Epoch [887/1000], Loss: 0.6417\n",
      "Epoch [888/1000], Loss: 0.6401\n",
      "Epoch [889/1000], Loss: 0.6391\n",
      "Epoch [890/1000], Loss: 0.6380\n",
      "Epoch [891/1000], Loss: 0.6367\n",
      "Epoch [892/1000], Loss: 0.6358\n",
      "Epoch [893/1000], Loss: 0.6345\n",
      "Epoch [894/1000], Loss: 0.6336\n",
      "Epoch [895/1000], Loss: 0.6323\n",
      "Epoch [896/1000], Loss: 0.6318\n",
      "Epoch [897/1000], Loss: 0.6311\n",
      "Epoch [898/1000], Loss: 0.6296\n",
      "Epoch [899/1000], Loss: 0.6281\n",
      "Epoch [900/1000], Loss: 0.6269\n",
      "Epoch [901/1000], Loss: 0.6264\n",
      "Epoch [902/1000], Loss: 0.6250\n",
      "Epoch [903/1000], Loss: 0.6241\n",
      "Epoch [904/1000], Loss: 0.6229\n",
      "Epoch [905/1000], Loss: 0.6218\n",
      "Epoch [906/1000], Loss: 0.6209\n",
      "Epoch [907/1000], Loss: 0.6199\n",
      "Epoch [908/1000], Loss: 0.6189\n",
      "Epoch [909/1000], Loss: 0.6179\n",
      "Epoch [910/1000], Loss: 0.6171\n",
      "Epoch [911/1000], Loss: 0.6162\n",
      "Epoch [912/1000], Loss: 0.6151\n",
      "Epoch [913/1000], Loss: 0.6140\n",
      "Epoch [914/1000], Loss: 0.6135\n",
      "Epoch [915/1000], Loss: 0.6127\n",
      "Epoch [916/1000], Loss: 0.6115\n",
      "Epoch [917/1000], Loss: 0.6101\n",
      "Epoch [918/1000], Loss: 0.6091\n",
      "Epoch [919/1000], Loss: 0.6082\n",
      "Epoch [920/1000], Loss: 0.6074\n",
      "Epoch [921/1000], Loss: 0.6064\n",
      "Epoch [922/1000], Loss: 0.6054\n",
      "Epoch [923/1000], Loss: 0.6043\n",
      "Epoch [924/1000], Loss: 0.6035\n",
      "Epoch [925/1000], Loss: 0.6025\n",
      "Epoch [926/1000], Loss: 0.6014\n",
      "Epoch [927/1000], Loss: 0.6004\n",
      "Epoch [928/1000], Loss: 0.5998\n",
      "Epoch [929/1000], Loss: 0.5987\n",
      "Epoch [930/1000], Loss: 0.5981\n",
      "Epoch [931/1000], Loss: 0.5972\n",
      "Epoch [932/1000], Loss: 0.5960\n",
      "Epoch [933/1000], Loss: 0.5955\n",
      "Epoch [934/1000], Loss: 0.5946\n",
      "Epoch [935/1000], Loss: 0.5932\n",
      "Epoch [936/1000], Loss: 0.5913\n",
      "Epoch [937/1000], Loss: 0.5892\n",
      "Epoch [938/1000], Loss: 0.5869\n",
      "Epoch [939/1000], Loss: 0.5838\n",
      "Epoch [940/1000], Loss: 0.5814\n",
      "Epoch [941/1000], Loss: 0.5778\n",
      "Epoch [942/1000], Loss: 0.5749\n",
      "Epoch [943/1000], Loss: 0.5715\n",
      "Epoch [944/1000], Loss: 0.5683\n",
      "Epoch [945/1000], Loss: 0.5659\n",
      "Epoch [946/1000], Loss: 0.5620\n",
      "Epoch [947/1000], Loss: 0.5598\n",
      "Epoch [948/1000], Loss: 0.5569\n",
      "Epoch [949/1000], Loss: 0.5541\n",
      "Epoch [950/1000], Loss: 0.5528\n",
      "Epoch [951/1000], Loss: 0.5500\n",
      "Epoch [952/1000], Loss: 0.5500\n",
      "Epoch [953/1000], Loss: 0.5475\n",
      "Epoch [954/1000], Loss: 0.5467\n",
      "Epoch [955/1000], Loss: 0.5465\n",
      "Epoch [956/1000], Loss: 0.5446\n",
      "Epoch [957/1000], Loss: 0.5435\n",
      "Epoch [958/1000], Loss: 0.5427\n",
      "Epoch [959/1000], Loss: 0.5422\n",
      "Epoch [960/1000], Loss: 0.5407\n",
      "Epoch [961/1000], Loss: 0.5386\n",
      "Epoch [962/1000], Loss: 0.5370\n",
      "Epoch [963/1000], Loss: 0.5365\n",
      "Epoch [964/1000], Loss: 0.5358\n",
      "Epoch [965/1000], Loss: 0.5339\n",
      "Epoch [966/1000], Loss: 0.5318\n",
      "Epoch [967/1000], Loss: 0.5314\n",
      "Epoch [968/1000], Loss: 0.5303\n",
      "Epoch [969/1000], Loss: 0.5290\n",
      "Epoch [970/1000], Loss: 0.5274\n",
      "Epoch [971/1000], Loss: 0.5261\n",
      "Epoch [972/1000], Loss: 0.5251\n",
      "Epoch [973/1000], Loss: 0.5234\n",
      "Epoch [974/1000], Loss: 0.5223\n",
      "Epoch [975/1000], Loss: 0.5211\n",
      "Epoch [976/1000], Loss: 0.5197\n",
      "Epoch [977/1000], Loss: 0.5182\n",
      "Epoch [978/1000], Loss: 0.5172\n",
      "Epoch [979/1000], Loss: 0.5158\n",
      "Epoch [980/1000], Loss: 0.5144\n",
      "Epoch [981/1000], Loss: 0.5131\n",
      "Epoch [982/1000], Loss: 0.5118\n",
      "Epoch [983/1000], Loss: 0.5109\n",
      "Epoch [984/1000], Loss: 0.5095\n",
      "Epoch [985/1000], Loss: 0.5087\n",
      "Epoch [986/1000], Loss: 0.5076\n",
      "Epoch [987/1000], Loss: 0.5063\n",
      "Epoch [988/1000], Loss: 0.5048\n",
      "Epoch [989/1000], Loss: 0.5044\n",
      "Epoch [990/1000], Loss: 0.5032\n",
      "Epoch [991/1000], Loss: 0.5016\n",
      "Epoch [992/1000], Loss: 0.5008\n",
      "Epoch [993/1000], Loss: 0.5000\n",
      "Epoch [994/1000], Loss: 0.4987\n",
      "Epoch [995/1000], Loss: 0.4977\n",
      "Epoch [996/1000], Loss: 0.4974\n",
      "Epoch [997/1000], Loss: 0.4963\n",
      "Epoch [998/1000], Loss: 0.4947\n",
      "Epoch [999/1000], Loss: 0.4941\n",
      "Epoch [1000/1000], Loss: 0.4932\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    # 数据转换\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y_area).to(torch.float)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23.4122], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([4,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "columns_name = ['x'+str(i) for i in range(9)]+['y']\n",
    "tic_data = pd.read_csv('tic_record.txt',names = columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0  x1  x2  x3  x4  x5  x6  x7  x8  y\n",
       "0   0   0   0   0   0   0   0   0   0  5\n",
       "1   0   0   0   0   0  -1   0   0   0  4\n",
       "2   0   0   0   0  -1   1   0   0   0  2\n",
       "3   0   0  -1   0   1  -1   0   0   0  8\n",
       "4   0   0   1   0  -1   1   0   0  -1  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选中的代码是从一个名为 tic_data 的 DataFrame 中提取特征矩阵 X 和目标向量 y。具体解释如下：\n",
    "\n",
    "X = tic_data.iloc[:,:9].values:\n",
    "\n",
    "iloc[:,:9] 表示选取 DataFrame 的所有行和前 9 列（索引从 0 开始）\n",
    ".values 将选取的数据转换为 NumPy 数组\n",
    "这行代码的作用是将前 9 列数据作为特征矩阵 X\n",
    "y = tic_data.iloc[:,9].values:\n",
    "\n",
    "iloc[:,9] 表示选取 DataFrame 的所有行和第 10 列（索引为 9）\n",
    ".values 将选取的数据转换为 NumPy 数组\n",
    "这行代码的作用是将第 10 列数据作为目标向量 y\n",
    "总结：这段代码通常用于机器学习任务中，将数据集分为特征矩阵 X 和目标变量 y，其中 X 包含前 9 列特征，y 包含第 10 列的目标值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tic_data.iloc[:,:9].values\n",
    "y = tic_data.iloc[:,9].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 9        # 输入层神经元数量，对应特征数量\n",
    "hidden_size = 20      # 隐藏层神经元数量\n",
    "output_size = 9       # 输出层神经元数量，对应分类类别数\n",
    "n_iter = 1000         # 训练迭代次数\n",
    "learning_rate = 0.01  # 学习率，控制参数更新步长\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()  # 调用父类nn.Module的初始化方法\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 定义输入层到隐藏层的全连接层\n",
    "        self.activate = nn.ReLU()  # 定义ReLU激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # 定义隐藏层到输出层的全连接层\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)  # 输入数据通过第一层全连接层\n",
    "        out = self.activate(out)  # 应用ReLU激活函数\n",
    "        out = self.fc2(out)  # 通过第二层全连接层\n",
    "        return out  # 返回最终输出\n",
    "\n",
    "    \n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, output_size)  # 初始化神经网络模型\n",
    "criterion = nn.CrossEntropyLoss()  # 定义交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/1000], Loss: 1.4286\n",
      "Accuracy of the network : 53.264685232857474 %\n",
      "Epoch [101/1000], Loss: 1.0437\n",
      "Accuracy of the network : 65.24689913409783 %\n",
      "Epoch [151/1000], Loss: 0.8821\n",
      "Accuracy of the network : 68.92113269365785 %\n",
      "Epoch [201/1000], Loss: 0.7988\n",
      "Accuracy of the network : 70.11467353147671 %\n",
      "Epoch [251/1000], Loss: 0.7561\n",
      "Accuracy of the network : 70.69974256962321 %\n",
      "Epoch [301/1000], Loss: 0.7347\n",
      "Accuracy of the network : 71.09758951556283 %\n",
      "Epoch [351/1000], Loss: 0.7192\n",
      "Accuracy of the network : 71.54224198455418 %\n",
      "Epoch [401/1000], Loss: 0.7066\n",
      "Accuracy of the network : 72.10390826117482 %\n",
      "Epoch [451/1000], Loss: 0.6942\n",
      "Accuracy of the network : 72.45494968406273 %\n",
      "Epoch [501/1000], Loss: 0.6825\n",
      "Accuracy of the network : 72.85279663000235 %\n",
      "Epoch [551/1000], Loss: 0.6741\n",
      "Accuracy of the network : 73.2272408144161 %\n",
      "Epoch [601/1000], Loss: 0.6674\n",
      "Accuracy of the network : 73.43786566814885 %\n",
      "Epoch [651/1000], Loss: 0.6626\n",
      "Accuracy of the network : 73.4612684296747 %\n",
      "Epoch [701/1000], Loss: 0.6585\n",
      "Accuracy of the network : 73.67189328340744 %\n",
      "Epoch [751/1000], Loss: 0.6548\n",
      "Accuracy of the network : 73.88251813714018 %\n",
      "Epoch [801/1000], Loss: 0.6519\n",
      "Accuracy of the network : 73.97612918324363 %\n",
      "Epoch [851/1000], Loss: 0.6492\n",
      "Accuracy of the network : 74.23355956002808 %\n",
      "Epoch [901/1000], Loss: 0.6471\n",
      "Accuracy of the network : 74.21015679850223 %\n",
      "Epoch [951/1000], Loss: 0.6456\n",
      "Accuracy of the network : 74.23355956002808 %\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(n_iter):\n",
    "    inputs = torch.from_numpy(X).to(torch.float)\n",
    "    targets = torch.from_numpy(y).to(torch.long)\n",
    "\n",
    "    # 前向过程\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # 后向过程\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 验证效果\n",
    "    if (epoch>0 and epoch%50==0):\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, n_iter, loss.item()))\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == targets).sum().item()\n",
    "            total = targets.size(0)\n",
    "        print('Accuracy of the network : {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "currentBoard = np.array([[0,0,0,0,0,0,0,0,0]])\n",
    "input = torch.from_numpy(currentBoard).to(torch.float)\n",
    "output = model(input)\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
